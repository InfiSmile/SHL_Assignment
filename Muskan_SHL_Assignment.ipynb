{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0botBbTN3vfpnpNiYebdY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InfiSmile/SHL_Assignment/blob/main/Muskan_SHL_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4_YcEOyV9Qj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text + Audio + Rules Ensemble\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xWIkmMMamyTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first, I just combined text and audio features and trained one model. Then I tried using only text models like Sentence Transformer and DeBERTa to see how well they perform, but that showed how important audio features actually are. I used NVIDIA’s Parakeet-TDT-0.6B-v2 model to transcribe the audios for better text data.\n",
        "\n",
        "Later, I explored few articles that I have mentioned later and found that ensembling can give better results. So, I built a setup where text, audio, and rule-based models are trained separately and then combined using NNLS and confidence-based blending. This way, each model contributes its strengths, text for meaning, audio for tone, and rules for structure ,making the final predictions more reliable.\n"
      ],
      "metadata": {
        "id": "RpKfQwcDWBcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Installation*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1.   !pip install -U transformers huggingface_hub\n",
        "2.   !pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PifMgVZGXw28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports and Config"
      ],
      "metadata": {
        "id": "4d2Azo82X_F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, math, warnings, random, re\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup\n",
        "\n",
        "#Ensemble Methods\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGB_AVAILABLE = True\n",
        "except Exception:\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    LGB_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "    SWA_AVAILABLE = True\n",
        "except Exception:\n",
        "    SWA_AVAILABLE = False\n",
        "\n",
        "#---For Audio------------\n",
        "import librosa, whisper\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "#Config\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TEXT_MODEL = \"microsoft/deberta-v3-small\"\n",
        "MAX_LEN = 256\n",
        "BS = 8\n",
        "EPOCHS_TEXT = 5\n",
        "EPOCHS_AUDIO = 8\n",
        "CLIP_RANGE = (0.0, 5.0)\n",
        "USE_ZSCORE = True\n",
        "AUDIO_SR = 16000\n",
        "\n",
        "# Paths\n",
        "TRAIN_CSV = \"/kaggle/input/dataset/train_transcripts.csv\"\n",
        "TEST_CSV  = \"/kaggle/input/dataset/test_transcripts.csv\"\n",
        "AUDIO_ROOT = \"/kaggle/input/shl-intern-hiring-assessment-2025/dataset/audios\"\n",
        "OUT_DIR = \"/kaggle/working/stacking_out\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "# ================================\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s);\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(s)\n",
        "\n",
        "set_seed()\n",
        "print(f\"Device: {DEVICE}\")\n"
      ],
      "metadata": {
        "id": "i-TgDEIsXmC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions\n",
        "$\\text{MAE} = \\frac{1}{N}\\sum_{i=1}^{N} |y_i - \\hat{y}_i|$\n",
        "\n",
        "$\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}$\n",
        "\n",
        "$r (Pearson correlation coefficient)  = \\frac{\\sum_{i=1}^{N} (y_i - \\bar{y})(\\hat{y}_i - \\bar{\\hat{y}})}\n",
        "{\\sqrt{\\sum_{i=1}^{N} (y_i - \\bar{y})^2} \\sqrt{\\sum_{i=1}^{N} (\\hat{y}_i - \\bar{\\hat{y}})^2}}$\n"
      ],
      "metadata": {
        "id": "V0EW9LZIYwYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def metrics(y, p):\n",
        "    mae = mean_absolute_error(y, p)\n",
        "    rmse = math.sqrt(mean_squared_error(y, p))\n",
        "    r = pearsonr(y, p)[0] if len(np.unique(y)) > 1 else np.nan\n",
        "    return mae, rmse, r\n",
        "\n",
        "#As mentioned in the assignment that it should be in the range 0 to 5\n",
        "def clip01_5(x):\n",
        "    return np.clip(x, CLIP_RANGE[0], CLIP_RANGE[1])\n",
        "\n",
        "def rank_scale(x: np.ndarray) -> np.ndarray:\n",
        "    idx = np.argsort(np.argsort(x))\n",
        "    return idx.astype(np.float32) / max(1, len(x)-1)\n"
      ],
      "metadata": {
        "id": "UX7tfNnbY0n5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transcript Preprocessing"
      ],
      "metadata": {
        "id": "1Qn1TJl4ZHgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(s: str) -> str:\n",
        "    s = re.sub(r\"\\b(\\w+)(\\s+\\1\\b)+\", r\"\\1\", s, flags=re.I)  # Since there are repeated tokens like I like I like\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "#Several disfluencies in the audio (explored few of them)\n",
        "DISFLUENCIES = {\"uh\",\"um\",\"erm\",\"hmm\",\"you know\",\"like\",\"sort of\"}\n",
        "\n",
        "def extract_rule_feats(texts: list[str]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for t in texts:\n",
        "        s = t or \"\"\n",
        "        tokens = s.split()\n",
        "        tok_n = len(tokens)\n",
        "        chars = len(s)\n",
        "        avg_tok = (chars / max(1, tok_n))\n",
        "        commas = s.count(\",\"); periods = s.count(\".\"); qmarks = s.count(\"?\"); exc = s.count(\"!\")\n",
        "        caps_ratio = sum(ch.isupper() for ch in s) / max(1, len(s))\n",
        "        repeats = sum(1 for i in range(1, tok_n) if tokens[i].lower()==tokens[i-1].lower())\n",
        "        disfluency_hits = sum(1 for w in DISFLUENCIES if w in s.lower())\n",
        "        rows.append(dict(\n",
        "            tok_n=tok_n, chars=chars, avg_tok=avg_tok,\n",
        "            commas=commas, periods=periods, qmarks=qmarks, exclam=exc,\n",
        "            caps_ratio=caps_ratio, repeats=repeats, disfluencies=disfluency_hits\n",
        "        ))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "## Rule-based feature engineering: These are later used by a simple regressor(Light GBM) for rule based model\n",
        "def enrich_rules(df):\n",
        "    df = df.copy()\n",
        "    df[\"punct_rate\"] = (df[\"commas\"]+df[\"periods\"]+df[\"qmarks\"]+df[\"exclam\"]) / np.maximum(1, df[\"tok_n\"])\n",
        "    df[\"repeat_rate\"] = df[\"repeats\"] / np.maximum(1, df[\"tok_n\"])\n",
        "    df[\"disfluency_rate\"] = df[\"disfluencies\"] / np.maximum(1, df[\"tok_n\"])\n",
        "    df[\"chars_per_tok\"] = df[\"chars\"] / np.maximum(1, df[\"tok_n\"])\n",
        "    df[\"caps_x_punct\"] = df[\"caps_ratio\"] * df[\"punct_rate\"]\n",
        "    df[\"avgTok_x_punct\"] = df[\"avg_tok\"] * df[\"punct_rate\"]\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "o9XPOYvQZG92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Audio Features"
      ],
      "metadata": {
        "id": "Er9yVZZwaAKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AudioFeaturizer:\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = whisper.load_model(\"tiny\", device=self.device)\n",
        "\n",
        "    def _encode(self, wav, sr):\n",
        "        if sr != 16000: #frequency\n",
        "            wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
        "        wav = whisper.pad_or_trim(torch.tensor(wav)) #Since whisper expects a fixed-length audio input.\n",
        "        #Convert waveform to log-Mel spectrogram for Whisper\n",
        "        mel = whisper.log_mel_spectrogram(wav).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            #Encoding features\n",
        "            hs = self.model.encoder(mel.unsqueeze(0))  # [1,T,384]\n",
        "        #averaging over time steps to obtain one fixed-size 384-dim audio embedding\n",
        "        return hs.squeeze(0).float().cpu().mean(dim=0)  # [384]\n",
        "\n",
        "    def __call__(self, wav_path: str):\n",
        "        ''' Load, normalize, and clean audio, then extract basic prosodic features (duration, RMS, ZCR) '''\n",
        "        wav, sr = librosa.load(wav_path, sr=AUDIO_SR, mono=True)\n",
        "        wav = librosa.util.normalize(wav)\n",
        "        wav, _ = librosa.effects.trim(wav, top_db=20)\n",
        "        dur = len(wav) / AUDIO_SR\n",
        "        rms = float(librosa.feature.rms(y=wav).mean())\n",
        "        zcr = float(librosa.feature.zero_crossing_rate(y=wav).mean())\n",
        "        # pitch features\n",
        "        try:\n",
        "            f0 = librosa.yin(wav, fmin=80, fmax=400, sr=AUDIO_SR)\n",
        "            f0 = f0[np.isfinite(f0)]\n",
        "            f0_mean = float(np.nanmean(f0)) if f0.size else 0.0\n",
        "            f0_std  = float(np.nanstd(f0))  if f0.size else 0.0\n",
        "            voiced_ratio = float(np.mean((f0 > 0).astype(float))) if f0.size else 0.0\n",
        "        except Exception:\n",
        "            f0_mean = f0_std = voiced_ratio = 0.0\n",
        "        enc = self._encode(wav, sr)  # [384]\n",
        "        vec = torch.cat([enc, torch.tensor([dur, rms, zcr, f0_mean, f0_std, voiced_ratio], dtype=torch.float32)], dim=0)  # [390]\n",
        "        return vec.numpy()\n"
      ],
      "metadata": {
        "id": "r71t4bdQaG1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Datasets\n",
        "\n",
        "\n",
        "1.   Text Model: \"microsoft/deberta-v3-small\"\n",
        "2.   Audio Model: openai- whisper tiny model\n",
        "\n"
      ],
      "metadata": {
        "id": "jCbcsl6DbWWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For loading transcripts\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts): self.texts = texts\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, i): return {\"text\": self.texts[i]}\n",
        "\n",
        "def text_collate(batch): return {\"text\": [b[\"text\"] for b in batch]}\n",
        "\n",
        "class TextRegressor(nn.Module):\n",
        "    '''  Text regression model using DeBERTa encoder and MLP head (CLS + mean pooled features)\n",
        "       [CLS] captures global sentence-level semantics learned during pretraining whereas,\n",
        "     Mean pooling adds information from all tokens, giving a more smoother context '''\n",
        "    def __init__(self, model_name=TEXT_MODEL):\n",
        "        super().__init__()\n",
        "        self.tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\n",
        "        self.txt = AutoModel.from_pretrained(model_name, trust_remote_code=False)\n",
        "        hid = self.txt.config.hidden_size\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(2*hid, 256), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(256, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, texts):\n",
        "        tok = self.tok(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
        "        out = self.txt(**tok).last_hidden_state  # [B,L,H]\n",
        "        cls  = out[:, 0]\n",
        "        mean = out.mean(dim=1)\n",
        "        emb  = torch.cat([cls, mean], dim=1)\n",
        "        return self.head(emb).squeeze(-1)\n",
        "\n",
        "#Since we would be using Layer-Wise Learning Rate Decay (LLRD) —\n",
        "#we need to assign a different learning rate to each transformer layer.\n",
        "def _get_layers(model):\n",
        "    #Check if the model has encoder.layer Like deBERTa\n",
        "    if hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):\n",
        "        return list(model.encoder.layer)\n",
        "    if hasattr(model, \"transformer\") and hasattr(model.transformer, \"layer\"):\n",
        "        return list(model.transformer.layer)\n",
        "    raise AttributeError(\"Unknown transformer layers path.\")\n",
        "\n",
        "#It helps fine-tune large pretrained models more safely like upper layers adapts to the new task\n",
        "#Lower layers changes slowly. So basically it shouldn't forget it's pretrained knowledge\n",
        "def llrd_params(model: TextRegressor, base_lr=3e-5, head_lr=1e-3, decay=0.9):\n",
        "    groups = [{\"params\": model.head.parameters(), \"lr\": head_lr}]\n",
        "    layers = _get_layers(model.txt)\n",
        "    lr = base_lr\n",
        "    for i in reversed(range(len(layers))):\n",
        "        groups.append({\"params\": layers[i].parameters(), \"lr\": lr})\n",
        "        lr *= decay\n",
        "    if hasattr(model.txt, \"embeddings\"):\n",
        "        groups.append({\"params\": model.txt.embeddings.parameters(), \"lr\": lr})\n",
        "    return groups\n",
        "\n",
        "#Small MLP model for audio embeddings which predicts a single regression value (as we need a value between 0 to 5)\n",
        "class AudioMLP(nn.Module):\n",
        "    def __init__(self, in_dim=390):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(512, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x).squeeze(-1)\n",
        "\n",
        "# R-Drop loss: combines supervised loss with consistency loss (for stability under dropout)\n",
        "def rdrop_loss(pred1, pred2, target, base_loss, alpha=2.0):\n",
        "    sup = base_loss(pred1, target) + base_loss(pred2, target)\n",
        "    cons = torch.mean((pred1 - pred2) ** 2)\n",
        "    return 0.5 * sup + alpha * cons\n",
        "\n",
        "# Maintains a moving average of model weights for smoother, more stable training\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "    def update(self, model):\n",
        "        with torch.no_grad():\n",
        "            for k, v in model.state_dict().items():\n",
        "                self.shadow[k].mul_((self.decay)).add_(v.detach(), alpha=1-self.decay)\n",
        "    def apply_to(self, model):\n",
        "        model.load_state_dict(self.shadow, strict=True)\n",
        "\n",
        "# It helps regularize the model . So basically it blends random pairs of samples and labels\n",
        "def mixup(x, y, alpha=0.2):\n",
        "    if alpha <= 0: return x, y\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    x_mix = lam * x + (1-lam) * x[idx]\n",
        "    y_mix = lam * y + (1-lam) * y[idx]\n",
        "    return x_mix, y_mix\n"
      ],
      "metadata": {
        "id": "Uld6nNPcbaYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loops"
      ],
      "metadata": {
        "id": "8QOk2jlFeerg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# - Train text regression model (DeBERTa + MLP) using gradual unfreezing + EMA + R-Drop\n",
        "# - Starts by freezing encoder layers, then unfreezes deeper ones each epoch\n",
        "# - Applies layer-wise learning rate decay (LLRD)\n",
        "# - Uses R-Drop for regularization and EMA for stable weight tracking\n",
        "def train_text(texts, y, idx_tr, idx_va, epochs=EPOCHS_TEXT):\n",
        "    model = TextRegressor().to(DEVICE)\n",
        "    for p in model.txt.parameters(): p.requires_grad = False\n",
        "\n",
        "    opt = torch.optim.AdamW(llrd_params(model, base_lr=3e-5, head_lr=1e-3, decay=0.9), weight_decay=1e-3)\n",
        "    loss_fn = nn.HuberLoss(delta=1.0)\n",
        "    steps = math.ceil(len(idx_tr)/BS) * epochs\n",
        "    sch = get_cosine_schedule_with_warmup(opt, int(0.1*steps), steps)\n",
        "    ema = EMA(model, decay=0.999)\n",
        "\n",
        "    mu = float(y[idx_tr].mean()); sigma = float(y[idx_tr].std() + 1e-6) if USE_ZSCORE else 1.0\n",
        "    def z(v): return (v - mu) / sigma if USE_ZSCORE else v\n",
        "    def uz(v): return v * sigma + mu if USE_ZSCORE else v\n",
        "\n",
        "    layers = _get_layers(model.txt)\n",
        "    for ep in range(epochs):\n",
        "        layers_to_unfreeze = min(2 + ep, len(layers))\n",
        "        for p in model.txt.parameters(): p.requires_grad = False\n",
        "        for i in range(len(layers) - layers_to_unfreeze, len(layers)):\n",
        "            for p in layers[i].parameters(): p.requires_grad = True\n",
        "        if hasattr(model.txt, \"embeddings\"):\n",
        "            for p in model.txt.embeddings.parameters(): p.requires_grad = False\n",
        "\n",
        "        model.train()\n",
        "        order = np.random.permutation(idx_tr)\n",
        "        for start in range(0, len(order), BS):\n",
        "            bix = order[start:start+BS]\n",
        "            b_texts = [texts[i] for i in bix]\n",
        "            b_labels = torch.tensor(z(y[bix]), dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            preds1 = model(b_texts)\n",
        "            preds2 = model(b_texts)\n",
        "            loss = rdrop_loss(preds1, preds2, b_labels, loss_fn, alpha=2.0)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step(); sch.step()\n",
        "            ema.update(model)\n",
        "\n",
        "    ema.apply_to(model)\n",
        "\n",
        "    model.eval()\n",
        "    va_texts = [texts[i] for i in idx_va]\n",
        "    va_preds = []\n",
        "    with torch.no_grad():\n",
        "        for s in range(0, len(va_texts), BS):\n",
        "            chunk = va_texts[s:s+BS]\n",
        "            p = model(chunk).detach().cpu().numpy()\n",
        "            va_preds.append(p)\n",
        "    va_preds = uz(np.concatenate(va_preds))\n",
        "    return va_preds, model, (mu, sigma)\n",
        "\n",
        "def train_audio(vecs, y, idx_tr, idx_va, epochs=EPOCHS_AUDIO):\n",
        "  ''' Train audio regression model (MLP) with MixUp '''\n",
        "    model = AudioMLP(in_dim=vecs.shape[1]).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "    loss_fn = nn.HuberLoss(delta=1.0)\n",
        "\n",
        "    mu = float(y[idx_tr].mean()); sigma = float(y[idx_tr].std() + 1e-6) if USE_ZSCORE else 1.0\n",
        "    def z(v): return (v - mu) / sigma if USE_ZSCORE else v\n",
        "    def uz(v): return v * sigma + mu if USE_ZSCORE else v\n",
        "\n",
        "    use_swa = SWA_AVAILABLE and (epochs >= 3)\n",
        "    if use_swa:\n",
        "        swa_model = AveragedModel(model)\n",
        "        swa_start = epochs - 3\n",
        "        swa_scheduler = SWALR(opt, swa_lr=5e-4)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        order = np.random.permutation(idx_tr)\n",
        "        for s in range(0, len(order), BS):\n",
        "            bix = order[s:s+BS]\n",
        "            bx = torch.tensor(vecs[bix], dtype=torch.float32, device=DEVICE)\n",
        "            by = torch.tensor(z(y[bix]), dtype=torch.float32, device=DEVICE)\n",
        "            bx, by = mixup(bx, by, alpha=0.15)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            pred = model(bx)\n",
        "            loss = loss_fn(pred, by)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            if use_swa and ep >= swa_start:\n",
        "                swa_model.update_parameters(model)\n",
        "                swa_scheduler.step()\n",
        "\n",
        "    if use_swa:\n",
        "        for p, sp in zip(model.parameters(), swa_model.parameters()):\n",
        "            p.data.copy_(sp.data)\n",
        "\n",
        "    model.eval()\n",
        "    va_preds = []\n",
        "    with torch.no_grad():\n",
        "        for s in range(0, len(idx_va), BS):\n",
        "            bix = idx_va[s:s+BS]\n",
        "            bx = torch.tensor(vecs[bix], dtype=torch.float32, device=DEVICE)\n",
        "            p = model(bx).detach().cpu().numpy()\n",
        "            va_preds.append(p)\n",
        "    va_preds = uz(np.concatenate(va_preds))\n",
        "    return va_preds, model, (mu, sigma)\n",
        "\n",
        "\n",
        "#Monte Carlo (MC) dropout inference for text model. Keep dropout active during inference.\n",
        "def mc_pred_text(model, texts, n=8):\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    for m in model.head.modules():\n",
        "        if isinstance(m, nn.Dropout): m.train()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n):\n",
        "            out = []\n",
        "            for i in range(0, len(texts), BS):\n",
        "                batch = texts[i:i+BS]\n",
        "                tokd = model.tok(batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
        "                hidden = model.txt(**tokd).last_hidden_state\n",
        "                cls, mean = hidden[:,0], hidden.mean(dim=1)\n",
        "                emb = torch.cat([cls, mean], dim=1)\n",
        "                p = model.head(emb).squeeze(-1).detach().cpu().numpy()\n",
        "                out.append(p)\n",
        "            preds.append(np.concatenate(out))\n",
        "    preds = np.stack(preds, 0)\n",
        "    return preds.mean(0), preds.std(0)\n",
        "\n",
        "#Monte Carlo (MC) dropout inference for audio model\n",
        "def mc_pred_audio(model, X, n=8):\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout): m.train()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n):\n",
        "            out = []\n",
        "            for i in range(0, len(X), BS):\n",
        "                b = torch.tensor(X[i:i+BS], dtype=torch.float32, device=DEVICE)\n",
        "                out.append(model(b).detach().cpu().numpy())\n",
        "            preds.append(np.concatenate(out))\n",
        "    preds = np.stack(preds, 0)\n",
        "    return preds.mean(0), preds.std(0)\n"
      ],
      "metadata": {
        "id": "20Zw1bLwemm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloading and Feature EXtraction"
      ],
      "metadata": {
        "id": "wcouyh_NfkBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "# Clean transcripts\n",
        "train_df[\"text\"] = train_df[\"text\"].map(clean_text)\n",
        "test_df[\"text\"]  = test_df[\"text\"].map(clean_text)\n",
        "\n",
        "y_all = train_df[\"label\"].values.astype(np.float32)\n",
        "\n",
        "print(\"Extracting rule features...\")\n",
        "train_rules = enrich_rules(extract_rule_feats(train_df[\"text\"].tolist()))\n",
        "test_rules  = enrich_rules(extract_rule_feats(test_df[\"text\"].tolist()))\n",
        "\n",
        "print(\"Extracting audio vectors \")\n",
        "fe = AudioFeaturizer(device=None)  # auto-select device\n",
        "def wav_path(mode, fn): return f\"{AUDIO_ROOT}/{mode}/{fn}.wav\"\n",
        "train_audio_vecs = np.stack([fe(wav_path(\"train\", fn)) for fn in train_df[\"filename\"]])  # [N,390]\n",
        "test_audio_vecs  = np.stack([fe(wav_path(\"test\",  fn)) for fn in test_df[\"filename\"]])\n",
        "# print(\"Shapes:\", train_audio_vecs.shape, test_audio_vecs.shape)\n"
      ],
      "metadata": {
        "id": "7IY89ulffm_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Fold Training\n",
        "\n",
        "\n",
        "1. The training loop performs 5-fold cross-validation, splitting data into train and validation sets in each fold.\n",
        "2. It trains separate text, audio, and rule-based models, generates out-of-fold predictions for validation, and also makes test predictions for each fold to later average for final results.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aqMpYAp8f1BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare stratified folds based on target distribution\n",
        "bins = pd.qcut(y_all, q=min(10, max(2, len(y_all)//30)), labels=False, duplicates=\"drop\")\n",
        "K = 5\n",
        "kf = StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)\n",
        "#Initialize arrays for out-of-fold (OOF) and test predictions\n",
        "oof_text  = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_audio = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_rules = np.zeros(len(train_df), dtype=np.float32)\n",
        "#Store predictions and uncertainties for each fold\n",
        "test_text_mean  = np.zeros((K, len(test_df)), dtype=np.float32)\n",
        "test_text_std   = np.zeros((K, len(test_df)), dtype=np.float32)\n",
        "test_audio_mean = np.zeros((K, len(test_df)), dtype=np.float32)\n",
        "test_audio_std  = np.zeros((K, len(test_df)), dtype=np.float32)\n",
        "test_rules_preds = np.zeros((K, len(test_df)), dtype=np.float32)\n",
        "\n",
        " #K-Fold training loop\n",
        "fold_idx = 0\n",
        "for tr_idx, va_idx in kf.split(train_df, bins):\n",
        "    fold_idx += 1\n",
        "    print(f\"\\n===== Fold {fold_idx}/{K} =====\")\n",
        "    y = y_all\n",
        "\n",
        "    # Train text regression model and get validation preds\n",
        "    text_va_pred, text_model, (mu_t, sig_t) = train_text(train_df[\"text\"].values, y, tr_idx, va_idx, epochs=EPOCHS_TEXT)\n",
        "    oof_text[va_idx] = text_va_pred\n",
        "\n",
        "    #  MC Dropout to get mean + std predictions for test set\n",
        "    mean_t, std_t = mc_pred_text(text_model, test_df[\"text\"].tolist(), n=8)\n",
        "    mean_t = mean_t * (sig_t if USE_ZSCORE else 1.0) + (mu_t if USE_ZSCORE else 0.0)\n",
        "    test_text_mean[fold_idx-1] = mean_t\n",
        "    test_text_std[fold_idx-1]  = std_t\n",
        "\n",
        "    # Train audio MLP model and get validation preds\n",
        "    audio_va_pred, audio_model, (mu_a, sig_a) = train_audio(train_audio_vecs, y, tr_idx, va_idx, epochs=EPOCHS_AUDIO)\n",
        "    oof_audio[va_idx] = audio_va_pred\n",
        "\n",
        "    #Run MC Dropout for test audio embeddings\n",
        "    mean_a, std_a = mc_pred_audio(audio_model, test_audio_vecs, n=8)\n",
        "    mean_a = mean_a * (sig_a if USE_ZSCORE else 1.0) + (mu_a if USE_ZSCORE else 0.0)\n",
        "    test_audio_mean[fold_idx-1] = mean_a\n",
        "    test_audio_std[fold_idx-1]  = std_a\n",
        "\n",
        "    # Rule based model\n",
        "    X_tr_rules = train_rules.iloc[tr_idx].values\n",
        "    y_tr = y[tr_idx].astype(float)\n",
        "    X_va = train_rules.iloc[va_idx].values\n",
        "    y_va = y[va_idx].astype(float)\n",
        "\n",
        "    if LGB_AVAILABLE:\n",
        "        feature_cols = list(train_rules.columns)\n",
        "        neg_names = {\"repeats\", \"disfluencies\", \"caps_ratio\", \"repeat_rate\", \"disfluency_rate\", \"caps_x_punct\"}\n",
        "        monotone_constraints = [(-1 if c in neg_names else 0) for c in feature_cols]\n",
        "\n",
        "        lgbm = lgb.LGBMRegressor(\n",
        "            n_estimators=1400,\n",
        "            learning_rate=0.02,\n",
        "            num_leaves=63,\n",
        "            min_child_samples=25,\n",
        "            subsample=0.85,\n",
        "            colsample_bytree=0.85,\n",
        "            reg_alpha=0.05,\n",
        "            reg_lambda=0.05,\n",
        "            random_state=SEED,\n",
        "            verbosity=-1,\n",
        "            monotone_constraints=monotone_constraints,\n",
        "        )\n",
        "        # Train LGBM and predict validation\n",
        "        lgbm.fit(\n",
        "            X_tr_rules, y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            eval_metric=\"l2\",\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=50),\n",
        "                lgb.log_evaluation(period=0),\n",
        "            ],\n",
        "        )\n",
        "        best_iter = getattr(lgbm, \"best_iteration_\", None)\n",
        "        oof_rules[va_idx] = lgbm.predict(X_va, num_iteration=best_iter)\n",
        "        test_rules_preds[fold_idx-1] = lgbm.predict(test_rules.values, num_iteration=best_iter)\n",
        "    else:\n",
        "        rf = RandomForestRegressor(n_estimators=500, random_state=SEED, n_jobs=-1)\n",
        "        rf.fit(X_tr_rules, y_tr)\n",
        "        oof_rules[va_idx] = rf.predict(X_va)\n",
        "        test_rules_preds[fold_idx-1] = rf.predict(test_rules.values)\n",
        "    #Combine text, audio, and rule-based predictions\n",
        "    base_stack = np.vstack([oof_text[va_idx], oof_audio[va_idx], oof_rules[va_idx]]).T\n",
        "    avg_pred = clip01_5(base_stack.mean(axis=1))\n",
        "    # Compute evaluation metrics for the current fold\n",
        "    mae, rmse, r = metrics(y[va_idx], avg_pred)\n",
        "    print(f\"Fold {fold_idx} base-avg -> MAE {mae:.3f} RMSE {rmse:.3f} r {r:.3f}\")\n"
      ],
      "metadata": {
        "id": "t4KZfe-xf4ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta Blending\n",
        "Got this idea reading this - [Meta Ensembling](https://medium.com/ml-research-lab/stacking-ensemble-meta-algorithms-for-improve-predictions-f4b4cf3b9237)\n",
        "After training individual text, audio, and rule-based models, this step performs meta-ensembling ,\n",
        "It combines their predictions intelligently.\n",
        "Using NNLS (Non-Negative Least Squares) and rank-based blending, Found the optimal weights for each model’s contribution.\n",
        "Then, isotonic regression is applied for calibration, ensuring final predictions are well-aligned and smooth.\n",
        "\n",
        "[Isotonic Regression](https://stats.stackexchange.com/questions/660622/why-isotonic-regression-for-model-calibration) -- I explored this while I was trying to improve my model and It worked.\n"
      ],
      "metadata": {
        "id": "CaaHcrHGh7ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# optimal ensemble weights using NNLS (Non-Negative Least Squares)\n",
        "def nnls_sum_to_one(X, y, iters=3000, lr=1e-2):\n",
        "    w = np.ones(X.shape[1], dtype=np.float32) / X.shape[1]\n",
        "    for _ in range(iters):\n",
        "        grad = (2.0 / len(y)) * (X.T @ (X @ w - y))\n",
        "        w = w - lr * grad\n",
        "        w = np.maximum(w, 0.0)\n",
        "        s = w.sum()\n",
        "        if s > 0: w /= s\n",
        "    return w\n",
        "\n",
        "# Stack model predictions (text, audio, rules)\n",
        "X_meta = np.vstack([oof_text, oof_audio, oof_rules]).T\n",
        "y_meta = y_all\n",
        "\n",
        "# Value based NNLS Ensemble :Finds the best non-negative combination of model outputs that minimizes prediction error.\n",
        "w_val = nnls_sum_to_one(X_meta, y_meta)\n",
        "oof_meta_val = clip01_5(X_meta @ w_val)\n",
        "print(\"\\nNNLS value weights:\", np.round(w_val, 4))\n",
        "mae, rmse, r = metrics(y_meta, oof_meta_val)\n",
        "print(f\"NNLS VALUE (OOF) -> MAE {mae:.3f} | RMSE {rmse:.3f} | r {r:.3f}\")\n",
        "\n",
        "#Rank based : Learns weights that best preserve the correct order of predictions.\n",
        "X_meta_rank = np.vstack([rank_scale(oof_text), rank_scale(oof_audio), rank_scale(oof_rules)]).T\n",
        "y_rank = rank_scale(y_meta)\n",
        "w_rank = nnls_sum_to_one(X_meta_rank, y_rank)\n",
        "oof_meta_rank = X_meta_rank @ w_rank\n",
        "print(\"NNLS rank weights:\", np.round(w_rank, 4))\n",
        "\n",
        "#MIximng up both Value based and Rank based\n",
        "alpha = 0.3\n",
        "oof_meta_blend = clip01_5((1 - alpha) * oof_meta_val + alpha * oof_meta_rank)\n",
        "mae, rmse, r = metrics(y_meta, oof_meta_blend)\n",
        "print(f\"RANK-BLEND OOF -> MAE {mae:.3f} | RMSE {rmse:.3f} | r {r:.3f}\")\n",
        "\n",
        "\n",
        "iso = IsotonicRegression(y_min=CLIP_RANGE[0], y_max=CLIP_RANGE[1], out_of_bounds=\"clip\")\n",
        "iso.fit(oof_meta_blend, y_meta)\n",
        "\n",
        "#Apply learned weights to test predictions\n",
        "test_text_m  = test_text_mean.mean(axis=0)\n",
        "test_text_s  = test_text_std.mean(axis=0) + 1e-6\n",
        "test_audio_m = test_audio_mean.mean(axis=0)\n",
        "test_audio_s = test_audio_std.mean(axis=0) + 1e-6\n",
        "test_rules_m = test_rules_preds.mean(axis=0)\n",
        "\n",
        "# Compute test-level weighted ensemble\n",
        "X_test_val = np.vstack([test_text_m, test_audio_m, test_rules_m]).T\n",
        "test_meta_val = clip01_5(X_test_val @ w_val)\n",
        "\n",
        "X_test_rank = np.vstack([rank_scale(test_text_m), rank_scale(test_audio_m), rank_scale(test_rules_m)]).T\n",
        "test_meta_rank = X_test_rank @ w_rank\n",
        "\n",
        "test_meta = clip01_5((1 - alpha) * test_meta_val + alpha * test_meta_rank)\n",
        "\n",
        "# giving more importance to models that are more confident\n",
        "w_conf = np.array([\n",
        "    1.0 / test_text_s.mean(),\n",
        "    1.0 / test_audio_s.mean(),\n",
        "    1.0,  # rules has no std\n",
        "], dtype=np.float32)\n",
        "w_conf = w_conf / w_conf.sum()\n",
        "conf_ens = clip01_5(w_conf[0]*test_text_m + w_conf[1]*test_audio_m + w_conf[2]*test_rules_m)\n",
        "\n",
        "beta = 0.25\n",
        "test_meta = clip01_5((1 - beta) * test_meta + beta * conf_ens)\n",
        "\n",
        "# Final standardization and isotonic calibration\n",
        "tm = (test_meta - test_meta.mean()) / (test_meta.std() + 1e-6)\n",
        "tm = tm * (y_meta.std() + 1e-6) + y_meta.mean()\n",
        "test_meta = clip01_5(0.7 * test_meta + 0.3 * tm)\n",
        "\n",
        "test_meta = clip01_5(iso.predict(test_meta))\n"
      ],
      "metadata": {
        "id": "zVWoIHFyh-Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING SUBMISSION.CSV"
      ],
      "metadata": {
        "id": "X_ibnDutlDNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pd.DataFrame({\n",
        "    \"filename\": test_df[\"filename\"],\n",
        "    \"label\": test_meta\n",
        "}).to_csv(os.path.join(OUT_DIR, \"stacked_predictions.csv\"), index=False)\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"oof_text\": oof_text,\n",
        "    \"oof_audio\": oof_audio,\n",
        "    \"oof_rules\": oof_rules,\n",
        "    \"oof_meta_value\": oof_meta_val,\n",
        "    \"oof_meta_rankblend\": oof_meta_blend,\n",
        "    \"label\": y_all,\n",
        "}).to_csv(os.path.join(OUT_DIR, \"oof_features.csv\"), index=False)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\" -\", os.path.join(OUT_DIR, \"stacked_predictions.csv\"))\n"
      ],
      "metadata": {
        "id": "rWQ0kBiTlFF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}